** GOAL
A module which allows for only a maxium number of connections to any of the
upstream servers. For example

  upstream mongrels {
    127.0.0.1:8000;
    127.0.0.1:8001;
    maxconn 2;
  }

will only allow 2 requests at a time to be sent to each Mongrel. That means if
there are more than 4 requests at a time, some will be queued inside this module
waiting for dispatch to a mongrel.

Why is this necessary? Many backends are programmed in crappy languages which
simply cannot handle the multiplexing. Mongrel is a perfect example - it is a
threaded HTTP server. That is, each request is done in a new Ruby thread.
Rails, the popular framework used with Mongrel, on the other hand has a large
mutex lock for processing each request. This means that Mongrel will accept
many connections but they wait in-line while Rails chugs along on a single
request. Having many open threads, many open sockets, and Rails chewing in the
background has proven to be a bad combination - it works - but experience shows
that a proxy like HAproxy with a maxconn=1 or maxconn=2 (i.e. the connections
queued in the proxy and not in the backends) performs much better.

** HOW TO ACHIVE THIS

Nginx provides a user friendly interface to add custom load balancers.
Unfortunately this interface assumes that every request is immediately sent
to a backend. To see why this is true let's look at the call stack when a
request goes through the server and hits a proxy_pass call. We start at the
proxy handler:

ngx_http_proxy_handler(request)
  ...
  ngx_http_read_client_request_body(request, ngx_http_upstream_init);
    ...
    ngx_http_upstream_init(request)
      ...                                       ////
      if (uscf->peer.init(r, uscf) != NGX_OK) { //// LOAD BALANCER PEER INIT
                                                ////
          ngx_http_finalize_request(r, NGX_HTTP_INTERNAL_SERVER_ERROR);
          return;
      }

      ngx_http_upstream_connect(request, request->upstream);
        ...
        ngx_event_connect_peer(request->upstream->peer)
          ...
          ////
          //// LOAD BALANCER FUNCTION
          ////
          request->upstream->peer->get( request->upstream->peer
                                      , request->upstream->peer->data
                                      )
          ...
          socket()
          ...
          connect()


Where uscf is of type ngx_http_upstream_srv_conf_t and equal to
request->upstream->conf->upstream.  Note that uscf->peer.init() is the peer
initialization function (http://emiller.info/nginx-modules-guide.html#lb-peer)

Note that the user of the load balancer module API does not have control of
whether ngx_http_upstream_connect is called.

This get() function is the "load balancer function" provided by the load
balancer module. It says which upstream server the request should be sent
to.
http://emiller.info/nginx-modules-guide.html#lb-function

After calling the load balancer callback ngx_event_connect_peer() proceeds
to the actual socket connection -- something we want to avoid if the all the
backends are currently at maxconn. Unfortunately once ngx_http_upstream_init()
gets called there are not any options which will allow us to delay an eventual
ngx_http_upstream_connect().


